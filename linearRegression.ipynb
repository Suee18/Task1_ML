{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mFailed to start the Kernel. \n",
            "\u001b[1;31mThe kernel died. Error: ... View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Nk3Rs959gNh",
        "outputId": "fb3abea8-2834-4eb8-9afb-1e4b8a9ab600"
      },
      "outputs": [],
      "source": [
        "\n",
        "# from google.colab import drive\n",
        "\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "# # Load the dataset\n",
        "# df = pd.read_csv('/content/drive/MyDrive/train.csv')\n",
        "\n",
        "# Load the dataset\n",
        "file_path = \"train.csv\"  # Ensure the file is in your working directory\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Display basic info\n",
        "print(df.info())\n",
        "print(df.head())\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Visualizing missing values\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.heatmap(df.isnull(), cmap='viridis', cbar=False, yticklabels=False)\n",
        "plt.title(\"Missing Values Heatmap\")\n",
        "plt.show()\n",
        "\n",
        "# Distribution of target variable (SalePrice)\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.histplot(df['SalePrice'], bins=30, kde=True)\n",
        "plt.title(\"SalePrice Distribution\")\n",
        "plt.xlabel(\"Sale Price\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()\n",
        "\n",
        "# Pairplot of selected numerical features (including SalePrice)\n",
        "selected_features = [\"SalePrice\", \"GrLivArea\", \"TotalBsmtSF\", \"YearBuilt\"]\n",
        "sns.pairplot(df[selected_features])\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XF05U7fd9gNi",
        "outputId": "6c343dd6-0489-4a23-aafe-61bad2138776"
      },
      "outputs": [],
      "source": [
        "# Check missing values percentage\n",
        "missing_data = df.isnull().sum() / len(df) * 100\n",
        "\n",
        "# Filter only columns with missing values\n",
        "missing_data = missing_data[missing_data > 0].sort_values(ascending=False)\n",
        "\n",
        "# Display missing values\n",
        "print(\"Missing Values (%):\")\n",
        "print(missing_data)\n",
        "\n",
        "# Bar Chart of Missing Values\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x=missing_data.index, y=missing_data.values, palette=\"viridis\")\n",
        "plt.xticks(rotation=90)\n",
        "plt.xlabel(\"Feature Name\")\n",
        "plt.ylabel(\"Percentage Missing\")\n",
        "plt.title(\"Percentage of Missing Data by Feature\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PtqUDT49gNi",
        "outputId": "d99db232-7aa3-456e-8fbc-b24ccae5fd55"
      },
      "outputs": [],
      "source": [
        "numerical_cols = [\"LotFrontage\", \"GarageYrBlt\", \"MasVnrArea\"]\n",
        "# ðŸ”¹ Histogram of numerical features before preprocessing\n",
        "df[numerical_cols].hist(figsize=(12, 6), bins=30, color='skyblue', edgecolor='black')\n",
        "plt.suptitle(\"Numerical Feature Distributions Before Preprocessing\")\n",
        "plt.show()\n",
        "\n",
        "# ðŸ”¹ Visualizing missing data before preprocessing\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.heatmap(df.isnull(), cmap='coolwarm', cbar=False, yticklabels=False)\n",
        "plt.title(\"Missing Data Heatmap - Before Preprocessing\")\n",
        "plt.show()\n",
        "#\n",
        "\n",
        "# Drop columns with too many missing values (>80%)\n",
        "columns_to_drop = [\"PoolQC\", \"MiscFeature\", \"Alley\", \"Fence\"]\n",
        "df.drop(columns=columns_to_drop, inplace=True)\n",
        "\n",
        "df.drop(columns=[\"Id\"], inplace=True)\n",
        "\n",
        "# Fill categorical missing values with \"None\"\n",
        "categorical_cols = [\"FireplaceQu\", \"GarageType\", \"GarageFinish\", \"GarageQual\", \"GarageCond\",\n",
        "                    \"BsmtExposure\", \"BsmtFinType2\", \"BsmtFinType1\", \"BsmtCond\", \"BsmtQual\", \"MasVnrType\"]\n",
        "df[categorical_cols] = df[categorical_cols].fillna(\"None\")\n",
        "\n",
        "# Fill numerical missing values with median\n",
        "for col in numerical_cols:\n",
        "    df[col] = df[col].fillna(df[col].median())\n",
        "\n",
        "# Fill Electrical with most frequent value\n",
        "df[\"Electrical\"] = df[\"Electrical\"].fillna(df[\"Electrical\"].mode()[0])\n",
        "\n",
        "# Verify no missing values remain\n",
        "print(f\"Total missing values after imputation: {df.isnull().sum().sum()}\")\n",
        "\n",
        "\n",
        "\n",
        "# ðŸ”¹ Visualizing missing data after preprocessing\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.heatmap(df.isnull(), cmap='coolwarm', cbar=False, yticklabels=False)\n",
        "plt.title(\"Missing Data Heatmap - After Preprocessing\")\n",
        "plt.show()\n",
        "\n",
        "# ðŸ”¹ Histogram of numerical features after preprocessing\n",
        "df[numerical_cols].hist(figsize=(12, 6), bins=30, color='lightcoral', edgecolor='black')\n",
        "plt.suptitle(\"Numerical Feature Distributions After Preprocessing\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Bar Chart for Categorical Features After Imputation\n",
        "plt.figure(figsize=(12, 6))\n",
        "df[categorical_cols].apply(lambda x: x.value_counts()).T.plot(kind=\"bar\", stacked=True, figsize=(12,6), colormap=\"viridis\")\n",
        "plt.title(\"Categorical Features - Value Distribution After Imputation\")\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCkr1cn69gNj",
        "outputId": "1bbadb27-4002-4e47-8347-14568bb90b2d"
      },
      "outputs": [],
      "source": [
        "\n",
        "# # Identify categorical features\n",
        "# categorical_features = df.select_dtypes(include=[\"object\"]).columns.tolist()\n",
        "\n",
        "# # Apply Label Encoding for ordinal categorical variables\n",
        "# ordinal_features = [\"BsmtQual\", \"BsmtCond\", \"GarageQual\", \"GarageCond\", \"FireplaceQu\"]\n",
        "# ordinal_mapping = {\n",
        "#     \"Ex\": 5, \"Gd\": 4, \"TA\": 3, \"Fa\": 2, \"Po\": 1, \"None\": 0\n",
        "# }\n",
        "\n",
        "# for col in ordinal_features:\n",
        "#     df[col] = df[col].map(ordinal_mapping)\n",
        "\n",
        "# # Apply One-Hot Encoding for nominal categorical features\n",
        "# df = pd.get_dummies(df, columns=[col for col in categorical_features if col not in ordinal_features], drop_first=True)\n",
        "\n",
        "# print(\"Encoding complete. Data shape:\", df.shape)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Identify categorical features\n",
        "categorical_features = df.select_dtypes(include=[\"object\"]).columns.tolist()\n",
        "\n",
        "# ðŸ”¹ Ordinal Encoding for Ordered Features\n",
        "ordinal_features = [\"BsmtQual\", \"BsmtCond\", \"GarageQual\", \"GarageCond\", \"FireplaceQu\",\n",
        "                    \"ExterQual\", \"ExterCond\", \"HeatingQC\"]\n",
        "\n",
        "ordinal_mapping = {\n",
        "    \"Ex\": 5, \"Gd\": 4, \"TA\": 3, \"Fa\": 2, \"Po\": 1, \"None\": 0\n",
        "}\n",
        "\n",
        "for col in ordinal_features:\n",
        "    df[col] = df[col].map(ordinal_mapping)\n",
        "\n",
        "# ðŸ”¹ Ordinal Encoding for BsmtExposure\n",
        "bsmt_exposure_mapping = {\"Gd\": 4, \"Av\": 3, \"Mn\": 2, \"No\": 1, \"None\": 0}\n",
        "df[\"BsmtExposure\"] = df[\"BsmtExposure\"].map(bsmt_exposure_mapping)\n",
        "\n",
        "# ðŸ”¹ Ordinal Encoding for GarageFinish\n",
        "garage_finish_mapping = {\"Fin\": 3, \"RFn\": 2, \"Unf\": 1, \"None\": 0}\n",
        "df[\"GarageFinish\"] = df[\"GarageFinish\"].map(garage_finish_mapping)\n",
        "\n",
        "# ðŸ”¹ One-Hot Encoding for Nominal Categorical Features\n",
        "nominal_features = [col for col in categorical_features if col not in ordinal_features and col not in [\"BsmtExposure\", \"GarageFinish\"]]\n",
        "\n",
        "df = pd.get_dummies(df, columns=nominal_features, drop_first=True)  # Drop first to avoid multicollinearity\n",
        "\n",
        "print(\"Encoding complete. Data shape:\", df.shape)\n",
        "\n",
        "# ðŸ”¹ Standardize Ordinal Encoded Features for Regression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "df[ordinal_features + [\"BsmtExposure\", \"GarageFinish\"]] = scaler.fit_transform(df[ordinal_features + [\"BsmtExposure\", \"GarageFinish\"]])\n",
        "\n",
        "print(\"Standardization complete.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ðŸ“Œ What This Improves\n",
        "âœ… Excludes SalePrice (target variable) from scaling\n",
        "âœ… Visualizes distributions before & after scaling (detects skewness)\n",
        "âœ… Boxplot helps confirm standardized features\n",
        "âœ… Preserves numerical feature relationship"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9SM_LBt9gNj",
        "outputId": "c471b878-c652-462b-ff10-ecfd7b545a60"
      },
      "outputs": [],
      "source": [
        "\n",
        "# # Identify numerical features\n",
        "# numerical_features = df.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
        "\n",
        "# # Apply Scaling\n",
        "# scaler = StandardScaler()\n",
        "# df[numerical_features] = scaler.fit_transform(df[numerical_features])\n",
        "\n",
        "# print(\"Feature scaling complete.\")\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# ðŸ”¹ Identify numerical features (excluding target variable if regression)\n",
        "numerical_features = df.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
        "\n",
        "# ðŸ”¹ Exclude target variable if it exists\n",
        "target_variable = \"SalePrice\"\n",
        "if target_variable in numerical_features:\n",
        "    numerical_features.remove(target_variable)\n",
        "\n",
        "# ðŸ”¹ Function to create histograms in multiple figures\n",
        "def plot_histograms(df, features, title):\n",
        "    num_features = len(features)\n",
        "    cols = 4  # Number of columns in the grid\n",
        "    rows = (num_features // cols) + 1  # Calculate the required rows\n",
        "\n",
        "    for i in range(0, num_features, cols * rows):\n",
        "        plt.figure(figsize=(15, 10))\n",
        "        sub_features = features[i : i + (cols * rows)]  # Batch the features\n",
        "        df[sub_features].hist(bins=30, figsize=(15, 10), layout=(rows, cols))\n",
        "        plt.suptitle(title)\n",
        "        plt.show()\n",
        "\n",
        "# ðŸ”¹ Visualizing distributions before scaling\n",
        "plot_histograms(df, numerical_features, \"Distributions Before Scaling\")\n",
        "\n",
        "# ðŸ”¹ Apply Standard Scaling (Z-score Normalization)\n",
        "scaler = StandardScaler()\n",
        "df[numerical_features] = scaler.fit_transform(df[numerical_features])\n",
        "\n",
        "# ðŸ”¹ Visualizing distributions after scaling\n",
        "plot_histograms(df, numerical_features, \"Distributions After Scaling\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 902
        },
        "id": "UTPYNXvt9gNk",
        "outputId": "69247b72-6d62-4c1b-a442-612492d73742"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# ðŸ”¹ Compute correlation with target variable\n",
        "correlation = df.corr()[\"SalePrice\"].abs().sort_values(ascending=False)\n",
        "\n",
        "# ðŸ”¹ Select features with correlation above 0.4 (adjust as needed)\n",
        "selected_features = correlation[correlation > 0.4].index.tolist()\n",
        "\n",
        "# ðŸ”¹ Ensure all selected features are numeric\n",
        "df[selected_features] = df[selected_features].apply(pd.to_numeric, errors=\"coerce\")\n",
        "\n",
        "# ðŸ”¹ Handle NaN and infinite values\n",
        "df[selected_features] = df[selected_features].replace([np.inf, -np.inf], np.nan)\n",
        "df = df.dropna()  # Drop rows with missing values\n",
        "\n",
        "# ðŸ”¹ Function to calculate VIF\n",
        "def calculate_vif(df, features):\n",
        "    vif_data = pd.DataFrame()\n",
        "    vif_data[\"Feature\"] = features\n",
        "    vif_data[\"VIF\"] = [variance_inflation_factor(df[features].values.astype(float), i) for i in range(len(features))]\n",
        "    return vif_data\n",
        "\n",
        "# Compute VIF\n",
        "vif_df = calculate_vif(df, selected_features)\n",
        "\n",
        "# Drop features with VIF > 10 (indicating high multicollinearity)\n",
        "low_vif_features = vif_df[vif_df[\"VIF\"] < 10][\"Feature\"].tolist()\n",
        "df = df[low_vif_features]\n",
        "\n",
        "# ðŸ”¹ Visualize top correlated features after selection\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.heatmap(df.corr(), annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
        "plt.title(\"Feature Correlation Matrix After Selection\")\n",
        "plt.show()\n",
        "print(\"Feature selection complete. Remaining features:\", df.columns)\n",
        "\n",
        "# ðŸ”¹ Bar Chart - Correlation of Selected Features with SalePrice\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x=correlation[low_vif_features].index, y=correlation[low_vif_features].values, palette=\"coolwarm\")\n",
        "plt.xticks(rotation=90)\n",
        "plt.xlabel(\"Features\")\n",
        "plt.ylabel(\"Correlation with SalePrice\")\n",
        "plt.title(\"Feature Importance Based on Correlation with SalePrice\")\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5jmFRtP9gNk",
        "outputId": "6f4768f9-870e-4bab-85d8-e0e00ffca0f3"
      },
      "outputs": [],
      "source": [
        "#splitting\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define features (X) and target variable (y)\n",
        "X = df.drop(columns=[\"SalePrice\"])  # All columns except target\n",
        "y = df[\"SalePrice\"]  # Target variable\n",
        "\n",
        "# First, split into Training (70%) and Temp (30%) sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Split Temp into Validation (15%) and Test (15%) sets\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "print(f\"Training set: X={X_train.shape}, y={y_train.shape[0]}\")\n",
        "print(f\"Validation set: X={X_val.shape}, y={y_val.shape[0]}\")\n",
        "print(f\"Test set: X={X_test.shape}, y={y_test.shape[0]}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Sb0tTNM9gNk",
        "outputId": "0df18813-a9b5-4253-e904-b1833d62d0bb"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Define model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Define k-fold cross-validation (k=5)\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Perform cross-validation and store scores\n",
        "rmse_scores = []\n",
        "mae_scores = []\n",
        "r2_scores = []\n",
        "\n",
        "for train_index, val_index in kf.split(X_train):\n",
        "    X_fold_train, X_fold_val = X_train.iloc[train_index], X_train.iloc[val_index]\n",
        "    y_fold_train, y_fold_val = y_train.iloc[train_index], y_train.iloc[val_index]\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_fold_train, y_fold_train)\n",
        "\n",
        "    # Predict on validation fold\n",
        "    y_pred = model.predict(X_fold_val)\n",
        "\n",
        "    # Compute metrics\n",
        "    rmse_scores.append(np.sqrt(mean_squared_error(y_fold_val, y_pred)))\n",
        "    mae_scores.append(mean_absolute_error(y_fold_val, y_pred))\n",
        "    r2_scores.append(r2_score(y_fold_val, y_pred))\n",
        "\n",
        "# Print average scores\n",
        "print(f\"Average RMSE: {np.mean(rmse_scores):.4f}\")\n",
        "print(f\"Average MAE: {np.mean(mae_scores):.4f}\")\n",
        "print(f\"Average RÂ² Score: {np.mean(r2_scores):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAhtQbzM9gNl",
        "outputId": "bf6ddb1d-d6c2-4f06-fe06-edf2572e9168"
      },
      "outputs": [],
      "source": [
        "# Train model on full training set\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_test_pred = model.predict(X_test)\n",
        "\n",
        "# Compute test set metrics\n",
        "test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
        "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
        "test_r2 = r2_score(y_test, y_test_pred)\n",
        "\n",
        "print(\"\\nFinal Model Performance on Test Set:\")\n",
        "print(f\"Test RMSE: {test_rmse:.4f}\")\n",
        "print(f\"Test MAE: {test_mae:.4f}\")\n",
        "print(f\"Test RÂ² Score: {test_r2:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVcbiN659gNl",
        "outputId": "0a059fce-82fd-4049-b2cd-df1d2c5fb82a"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    \"alpha\": [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]  # Regularization strength\n",
        "}\n",
        "\n",
        "# Ridge Regression\n",
        "ridge_model = Ridge()\n",
        "ridge_grid = GridSearchCV(ridge_model, param_grid, scoring=\"neg_root_mean_squared_error\", cv=5, n_jobs=-1)\n",
        "ridge_grid.fit(X_train, y_train)\n",
        "\n",
        "# Lasso Regression\n",
        "lasso_model = Lasso()\n",
        "lasso_grid = GridSearchCV(lasso_model, param_grid, scoring=\"neg_root_mean_squared_error\", cv=5, n_jobs=-1)\n",
        "lasso_grid.fit(X_train, y_train)\n",
        "\n",
        "# Elastic Net Regression\n",
        "elastic_model = ElasticNet()\n",
        "elastic_grid = GridSearchCV(elastic_model, {\"alpha\": [0.0001, 0.001, 0.01, 0.1, 1, 10]}, scoring=\"neg_root_mean_squared_error\", cv=5, n_jobs=-1)\n",
        "elastic_grid.fit(X_train, y_train)\n",
        "\n",
        "# Print best parameters\n",
        "print(f\"Best Ridge Alpha: {ridge_grid.best_params_['alpha']}\")\n",
        "print(f\"Best Lasso Alpha: {lasso_grid.best_params_['alpha']}\")\n",
        "print(f\"Best Elastic Net Alpha: {elastic_grid.best_params_['alpha']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0AyTxjCA9gNm",
        "outputId": "cfb5c5de-110a-42a4-c7a4-a7c8f987e86a"
      },
      "outputs": [],
      "source": [
        "# Choose best model (Ridge as an example)\n",
        "best_model = Ridge(alpha=ridge_grid.best_params_['alpha'])\n",
        "best_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_test_pred = best_model.predict(X_test)\n",
        "\n",
        "# Compute test set metrics\n",
        "test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
        "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
        "test_r2 = r2_score(y_test, y_test_pred)\n",
        "\n",
        "print(\"\\nFinal Tuned Model Performance on Test Set:\")\n",
        "print(f\"Test RMSE: {test_rmse:.4f}\")\n",
        "print(f\"Test MAE: {test_mae:.4f}\")\n",
        "print(f\"Test RÂ² Score: {test_r2:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5VOpBqsG9gNm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xFvZzYZ9gNm"
      },
      "source": [
        "Visualising the learning curve\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "3b78cg4T9gNm",
        "outputId": "3ef5ab67-61e0-4c25-b340-dd6ee92dea32"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import learning_curve\n",
        "\n",
        "# Define model\n",
        "model = Ridge(alpha=ridge_grid.best_params_['alpha'])\n",
        "\n",
        "# Get learning curve\n",
        "train_sizes, train_scores, val_scores = learning_curve(\n",
        "    model, X_train, y_train, cv=5, scoring=\"neg_mean_squared_error\", train_sizes=np.linspace(0.1, 1.0, 10)\n",
        ")\n",
        "\n",
        "# Compute mean loss\n",
        "train_mean = -np.mean(train_scores, axis=1)\n",
        "val_mean = -np.mean(val_scores, axis=1)\n",
        "\n",
        "# Plot Learning Curve\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(train_sizes, train_mean, label=\"Training Loss (MSE)\")\n",
        "plt.plot(train_sizes, val_mean, label=\"Validation Loss (MSE)\", linestyle=\"dashed\")\n",
        "plt.xlabel(\"Training Set Size\")\n",
        "plt.ylabel(\"Mean Squared Error (MSE)\")\n",
        "plt.title(\"Learning Curve\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "zgdHidI29gNn",
        "outputId": "9e285fc7-8919-4ef8-b60e-d435e8535edb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "# Convert DataFrame values to numeric\n",
        "X_train = X_train.astype(float)\n",
        "y_train = y_train.astype(float)\n",
        "X_val = X_val.astype(float)\n",
        "y_val = y_val.astype(float)\n",
        "\n",
        "# Initialize parameters\n",
        "def initialize_weights(n_features):\n",
        "    w = np.zeros(n_features)  # Weights\n",
        "    b = 0  # Bias\n",
        "    return w, b\n",
        "\n",
        "# Compute MSE Loss\n",
        "def compute_mse(y_true, y_pred):\n",
        "    return np.mean((y_true - y_pred) ** 2)\n",
        "\n",
        "# Gradient Descent Implementation\n",
        "def train_linear_regression(X_train, y_train, X_val, y_val, learning_rate=0.01, epochs=100):\n",
        "    n_samples, n_features = X_train.shape\n",
        "    w, b = initialize_weights(n_features)\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Compute Predictions\n",
        "        y_train_pred = np.dot(X_train, w) + b\n",
        "        y_val_pred = np.dot(X_val, w) + b\n",
        "\n",
        "        # Compute Loss\n",
        "        train_loss = compute_mse(y_train, y_train_pred)\n",
        "        val_loss = compute_mse(y_val, y_val_pred)\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        # Compute Gradients\n",
        "        dw = -(2 / n_samples) * np.dot(X_train.T, (y_train - y_train_pred))\n",
        "        db = -(2 / n_samples) * np.sum(y_train - y_train_pred)\n",
        "\n",
        "        # Update Weights\n",
        "        w -= learning_rate * dw\n",
        "        b -= learning_rate * db\n",
        "\n",
        "    return train_losses, val_losses\n",
        "\n",
        "# Train the model using gradient descent\n",
        "train_losses, val_losses = train_linear_regression(X_train.values, y_train.values, X_val.values, y_val.values)\n",
        "\n",
        "# Plot Training and Validation Loss\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(range(len(train_losses)), train_losses, label=\"Training Loss (MSE)\")\n",
        "plt.plot(range(len(val_losses)), val_losses, label=\"Validation Loss (MSE)\", linestyle=\"dashed\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Mean Squared Error (MSE)\")\n",
        "plt.title(\"Loss Function over Epochs\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "FBw04q5C9gNn",
        "outputId": "d926a164-429a-4967-b829-9a9c229da3d2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Sample Data (Ensure X and y are properly defined)\n",
        "X = np.random.rand(100, 1) * 100  # 100 random values\n",
        "y = 3.5 * X.squeeze() + np.random.randn(100) * 10  # Linear relationship with noise\n",
        "\n",
        "# Split Data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Ensure `y_test` and `y_pred` are the same length\n",
        "if len(y_test) != len(y_pred):\n",
        "    y_test = y_test[:len(y_pred)]  # Adjust if necessary\n",
        "\n",
        "# Plot Actual vs Predicted Values\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.scatter(y_test, y_pred, alpha=0.5, color=\"blue\")\n",
        "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle=\"--\", linewidth=2)\n",
        "plt.xlabel(\"Actual Prices\")\n",
        "plt.ylabel(\"Predicted Prices\")\n",
        "plt.title(\"Actual vs. Predicted Values\")\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "8Se6Q7vE9gNn",
        "outputId": "f1d6f166-48e1-410a-eb66-22efafff509b"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Sample Data (Ensure X and y are properly defined)\n",
        "X = np.random.rand(100, 1) * 100  # 100 random values\n",
        "y = 3.5 * X.squeeze() + np.random.randn(100) * 10  # Linear relationship with noise\n",
        "\n",
        "# Split Data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Compute Residuals\n",
        "residuals = y_test - y_pred  # Difference between actual and predicted values\n",
        "\n",
        "# Plot Residuals Histogram\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.hist(residuals, bins=20, edgecolor='black', alpha=0.7)\n",
        "plt.axvline(0, color='red', linestyle=\"--\", linewidth=2)  # Reference line at zero\n",
        "plt.xlabel(\"Residuals (Errors)\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Distribution of Residuals (Errors)\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        },
        "id": "-_XlhWS89gNn",
        "outputId": "a5fd25bc-8120-4306-a0cb-c9866ff1f8fc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Generate Non-Linear Data\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 1) * 10  # Feature: Random values between 0 and 10\n",
        "y = 3 * (X**2) + 2 * X + 5 + np.random.randn(100, 1) * 5  # Quadratic relationship with noise\n",
        "\n",
        "# Split Data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Polynomial Regression (Degree 2)\n",
        "degree = 2  # Change this for higher-degree polynomials\n",
        "model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate Model\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Polynomial Regression (Degree {degree})\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse:.3f}\")\n",
        "print(f\"RÂ² Score: {r2:.3f}\")\n",
        "\n",
        "# Plot Actual vs Predicted\n",
        "plt.scatter(X_test, y_test, color=\"blue\", label=\"Actual Data\", alpha=0.6)\n",
        "plt.scatter(X_test, y_pred, color=\"red\", label=\"Predictions\", alpha=0.6)\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.title(f\"Polynomial Regression (Degree {degree}) Predictions\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwjdA374ASrO",
        "outputId": "d623a3fa-d332-4fd9-aa78-947a80ed637f"
      },
      "outputs": [],
      "source": [
        "!git commit -m \"Added my updates from Colab\"\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
